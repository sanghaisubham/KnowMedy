{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  The/DT\n",
      "  (Chunk incredibly/RB intimidating/VBG NLP/NNP)\n",
      "  scares/NNS\n",
      "  people/NNS\n",
      "  away/RB\n",
      "  who/WP\n",
      "  are/VBP\n",
      "  sissies/NNS)\n"
     ]
    }
   ],
   "source": [
    "# POS tag list:\n",
    "\n",
    "#     CC coordinating conjunction\n",
    "#     CD cardinal digit\n",
    "#     DT determiner\n",
    "#     EX existential there (like: \"there is\" ... think of it like \"there exists\")\n",
    "#     FW foreign word\n",
    "#     IN preposition/subordinating conjunction\n",
    "#     JJ adjective 'big'\n",
    "#     JJR adjective, comparative 'bigger'\n",
    "#     JJS adjective, superlative 'biggest'\n",
    "#     LS list marker 1)\n",
    "#     MD modal could, will\n",
    "#     NN noun, singular 'desk'\n",
    "#     NNS noun plural 'desks'\n",
    "#     NNP proper noun, singular 'Harrison'\n",
    "#     NNPS proper noun, plural 'Americans'\n",
    "#     PDT predeterminer 'all the kids'\n",
    "#     POS possessive ending parent's\n",
    "#     PRP personal pronoun I, he, she\n",
    "#     PRP$ possessive pronoun my, his, hers\n",
    "#     RB adverb very, silently,\n",
    "#     RBR adverb, comparative better\n",
    "#     RBS adverb, superlative best\n",
    "#     RP particle give up\n",
    "#     TO to go 'to' the store.\n",
    "#     UH interjection errrrrrrrm\n",
    "#     VB verb, base form take\n",
    "#     VBD verb, past tense took\n",
    "#     VBG verb, gerund/present participle taking\n",
    "#     VBN verb, past participle taken\n",
    "#     VBP verb, sing. present, non-3d take\n",
    "#     VBZ verb, 3rd person sing. present takes\n",
    "#     WDT wh-determiner which\n",
    "#     WP wh-pronoun who, what\n",
    "#     WP$ possessive wh-pronoun whose\n",
    "#     WRB wh-abverb where, when\n",
    "\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "#nltk.download()\n",
    "# import CookieJar\n",
    "# cj = CookieJar()\n",
    "# opener = urllib2.build_opener(urllib2.HTTPCookieProcessor(cj))\n",
    "# opener.addheaders = [('User-agent', 'Mozilla/5.0')]\n",
    "\n",
    "\n",
    "# #empty array#\n",
    "# contentArray =['Starbucks is doing very well lately.',\n",
    "#                'Overall, while it may seem there is already a Starbucks on every corner, Starbucks still has a lot of room to grow.',\n",
    "#                'They just began expansion into food products, which has been going quite well so far for them.',\n",
    "#                'I can attest that my own expenditure when going to Starbucks has increased, in lieu of these food products.',\n",
    "#                'Starbucks is also indeed expanding their number of stores as well.',\n",
    "#                'Starbucks still sees strong sales growth here in the united states, and intends to actually continue increasing this.',\n",
    "#                'Starbucks also has one of the more successful loyalty programs, which accounts for 30%  of all transactions being loyalty-program-based.',\n",
    "#                'As if news could not get any more positive for the company, Brazilian weather has become ideal for producing coffee beans.',\n",
    "#                'Brazil is the world\\'s #1 coffee producer, the source of about 1/3rd of the entire world\\'s supply!',\n",
    "#                'Given the dry weather, coffee farmers have amped up production, to take as much of an advantage as possible with the dry weather.',\n",
    "#                'Increase in supply... well you know the rules...',]\n",
    "\n",
    "\n",
    "exampleArray = ['The incredibly intimidating NLP scares people away who are sissies']\n",
    "               \n",
    "\n",
    "def processContent():\n",
    "    try:\n",
    "        for item in exampleArray:\n",
    "            tokenized = nltk.word_tokenize(item)\n",
    "            tagged = nltk.pos_tag(tokenized)\n",
    "\n",
    "            chunkGram = r\"\"\"Chunk: {<RB.?>*<VB.?>*<NNP>}\"\"\"\n",
    "            chunkParser = nltk.RegexpParser(chunkGram)\n",
    "\n",
    "            chunked = chunkParser.parse(tagged)\n",
    "            print(chunked)\n",
    "            chunked.draw()\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "    \n",
    "\n",
    "\n",
    "processContent()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    }
   ],
   "source": [
    "from nltk.book import *\n",
    "x='Welcome to my Notebook'\n",
    "tokenized=nltk.word_tokenize(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Welcome', 'to', 'my', 'Notebook']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tagged=nltk.pos_tag(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (Chunk The/DT)\n",
      "  incredibly/RB\n",
      "  (Chunk intimidating/VBG)\n",
      "  (Chunk NLP/NNP)\n",
      "  scares/NNS\n",
      "  people/NNS\n",
      "  away/RB\n",
      "  (Chunk who/WP)\n",
      "  (Chunk are/VBP)\n",
      "  sissies/NNS)\n"
     ]
    }
   ],
   "source": [
    "tokenized = nltk.word_tokenize(exampleArray[0])\n",
    "tagged = nltk.pos_tag(tokenized)\n",
    "chunkgram=r\"\"\"\n",
    "    Chunk:\n",
    "        {<.*>}\n",
    "        }<RB|NNS>{ \n",
    "        \n",
    "        \"\"\"#Remove every adverb and plural noun\n",
    "chunkParser = nltk.RegexpParser(chunkgram)\n",
    "chunked = chunkParser.parse(tagged)\n",
    "print(chunked)\n",
    "chunked.draw()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "namedEnt=nltk.ne_chunk(tagged,binary=True)\n",
    "namedEnt.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenized = nltk.word_tokenize('High Fever')\n",
    "tagged=nltk.pos_tag(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'High'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing  :sweating VBG\n",
      "VAL1 :v\n",
      "sweat\n",
      "Printing  :increase NN\n",
      "VAL1 :n\n",
      "increase\n",
      "['sweat', 'increase']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "lemmatiser = WordNetLemmatizer()\n",
    "data = 'sweating increase'\n",
    "tokenized = nltk.word_tokenize(data)\n",
    "tagged=nltk.pos_tag(tokenized)\n",
    "mydata=''\n",
    "for i in tagged:\n",
    "    print('Printing  :' +i[0]+' '+i[1])\n",
    "    val1=''\n",
    "    if(i[1][0]=='V'):\n",
    "        val1=\"v\"\n",
    "    else:\n",
    "        val1=\"n\"\n",
    "    print(\"VAL1 :\"+val1)\n",
    "    val=(lemmatiser.lemmatize(i[0], pos=val1))\n",
    "    print(val)\n",
    "    mydata=mydata+' '+val\n",
    "stopWords = set(stopwords.words('english'))\n",
    "words = word_tokenize(mydata)\n",
    "wordsFiltered = []\n",
    " \n",
    "for w in words:\n",
    "    if w not in stopWords:\n",
    "        wordsFiltered.append(w)\n",
    "print(wordsFiltered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatise studying: study\n"
     ]
    }
   ],
   "source": [
    "print(\"Lemmatise %s: %s\" % (\"studying\", lemmatiser.lemmatize(\"studying\", pos=\"v\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "high\n",
      "JJ\n",
      "2\n",
      "fever\n",
      "NN\n",
      "'Tree' object has no attribute 'parse'\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import treebank_chunk\n",
    "exampleArray=['high fever','red eye','very high fever']\n",
    "def processContent():\n",
    "    try:\n",
    "        for item in exampleArray:\n",
    "            tokenized = nltk.word_tokenize(item)\n",
    "            tagged = nltk.pos_tag(tokenized)\n",
    "            chunkGram = r\"\"\"Chunk: {<RB.?>*<VB.?>|<JJ.?>*<NN.?>}\"\"\"\n",
    "            chunkParser = nltk.RegexpParser(chunkGram)\n",
    "\n",
    "            chunked = chunkParser.parse(tagged)\n",
    "            def traverse_tree(tree):\n",
    "                for subtree in tree:\n",
    "                    if type(subtree)==nltk.tree.Tree:\n",
    "                        traverse_tree(subtree)\n",
    "                    else:\n",
    "                        #print(len(subtree))\n",
    "                        if subtree[1]=='NN':\n",
    "                            val=\n",
    "            traverse_tree(chunked)\n",
    "#             tree=treebank_chunk.chunked_sents()[0]\n",
    "#             print(tree)\n",
    "#             print(chunked[0])\n",
    "#             print('\\n')\n",
    "#             print(chunked[1])\n",
    "\n",
    "            brown=nltk.corpus.brown\n",
    "            for sent in brown.tagged_sents():\n",
    "                tree=chunked.parse(sent)\n",
    "                for subtree in tree.subtrees():\n",
    "                    if subtree.label()=='CHUNK':\n",
    "                        print(subtree)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "processContent()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WordNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "syns = wordnet.synsets('in')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(syns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('inch.n.01'),\n",
       " Synset('indium.n.01'),\n",
       " Synset('indiana.n.01'),\n",
       " Synset('in.s.01'),\n",
       " Synset('in.s.02'),\n",
       " Synset('in.s.03'),\n",
       " Synset('in.r.01')]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inch.n.01\n"
     ]
    }
   ],
   "source": [
    "print(syns[0].name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run\n"
     ]
    }
   ],
   "source": [
    "print(syns[0].lemmas()[0].name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Lemma('run.n.05.run'),\n",
       " Lemma('run.n.05.running'),\n",
       " Lemma('run.n.05.running_play'),\n",
       " Lemma('run.n.05.running_game')]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syns[0].lemmas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
